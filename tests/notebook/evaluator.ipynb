{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/web_services/galaxy/jupyter_conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "Keras = tf.keras\n",
    "model_from_json = Keras.models.model_from_json\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, roc_auc_score\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import warnings\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# caused by divide by zero during metrics calcultiong\n",
    "# ignoring  because it is saturating the error logs\n",
    "warnings.filterwarnings(action='ignore', category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_json, weights_path):\n",
    "    model = model_from_json(model_json)\n",
    "    model.load_weights(weights_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def rounded_str(num, precision=6):\n",
    "    if type(num) is str:\n",
    "        return num\n",
    "    return str(round(num, precision))\n",
    "\n",
    "\n",
    "def get_best_threshold(train_x, train_y, model, delta=0.005):        \n",
    "    curr_thresh = 0.0\n",
    "    best_thresh = 0.0\n",
    "    best_gmean = 0.0  \n",
    "    \n",
    "    y_prob = model.predict(train_x)\n",
    "\n",
    "    while True:\n",
    "        y_pred = np.where(y_prob > curr_thresh, np.ones_like(y_prob), np.zeros_like(y_prob))\n",
    "        tn, fp, fn, tp = confusion_matrix(train_y, y_pred).ravel()\n",
    "        tpr = (tp) / (tp + fn)\n",
    "        tnr = (tn) / (tn + fp)\n",
    "        if tnr > tpr:\n",
    "            return best_thresh\n",
    "        gmean = math.sqrt(tpr*tnr)\n",
    "        if gmean > best_gmean:\n",
    "            best_gmean = gmean\n",
    "            best_thresh = curr_thresh\n",
    "        curr_thresh += delta\n",
    "        print(curr_thresh)\n",
    "        \n",
    "    return best_thresh\n",
    "\n",
    "\n",
    "\n",
    "columns = ['minority_size', 'strategy', 'tp', 'fp', 'tn', 'fn', 'tpr', 'tnr', 'roc_auc', 'geometric_mean', 'arithmetic_mean', 'f1_score', 'precision']\n",
    "\n",
    "\n",
    "def write_performance_metrics(y_true, y_prob, minority_size, strategy, threshold, path):\n",
    "    # include header if file doesn't exist yet\n",
    "    out = \",\".join(columns) if not os.path.isfile(path) else ''\n",
    "\n",
    "    predictions = np.where(y_prob > threshold, 1.0, 0.0)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, predictions).ravel()\n",
    "    tpr = (tp) / (tp + fn)\n",
    "    tnr = (tn) / (tn + fp)\n",
    "    roc_auc = roc_auc_score(y_true, y_prob)\n",
    "    geometric_mean = math.sqrt(tpr * tnr)\n",
    "    arithmetic_mean = 0.5 * (tpr + tnr)\n",
    "    f1 = f1_score(y_true, predictions)\n",
    "    precision = precision_score(y_true, predictions)\n",
    "\n",
    "    results = [minority_size, strategy, tp, fp, tn, fn, tpr, tnr, roc_auc, geometric_mean, arithmetic_mean, f1, precision]\n",
    "    results = [rounded_str(x) for x in results]\n",
    "    out += '\\n' + ','.join(results)\n",
    "\n",
    "    with open(path, 'a') as outfile:\n",
    "        outfile.write(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input config\n",
    "data_file = '/home/jjohn273/git/ECBDL14-Classification/data/ecbdl14.onehot.sample.hdf'\n",
    "test_key = 'test'\n",
    "train_key = 'train'\n",
    "models_dir = '/home/jjohn273/git/ECBDL14-Classification/tests/trained-models/'\n",
    "\n",
    "# output config\n",
    "results_file = 'results.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_hdf(data_file, test_key)\n",
    "test_y, test_x = test_data['target'], test_data.drop(columns=['target'])\n",
    "\n",
    "# training data will be used to estimate optimal thresholds\n",
    "train_data = pd.read_hdf(data_file, train_key)\n",
    "train_y, train_x = train_data['target'], train_data.drop(columns=['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Models to Be Evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [dir_name for dir_name in os.listdir(models_dir) if 'group' in dir_name]\n",
    "group_paths = [os.path.join(models_dir, g) for g in groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model architecture\n",
    "model_path = os.path.join(group_paths[0], 'model-architecture.json')\n",
    "with open(model_path, 'r') as json_in:\n",
    "    model_json = json_in.read()\n",
    "    \n",
    "\n",
    "# build results set\n",
    "counts = {}\n",
    "trained_models = []\n",
    "for group_path in group_paths:\n",
    "    weight_files = [path for path in os.listdir(os.path.join(group_path, 'weights')) if 'model.h5' in path]\n",
    "    results = ([(*f.split('-')[:2], os.path.join(group_path, 'weights', f)) for f in weight_files])\n",
    "    trained_models.extend(results)\n",
    "    for (pos_size, run, weights_path) in results:\n",
    "        counts[pos_size] = counts.get(pos_size, 0) + 1\n",
    "\n",
    "trained_models = pd.DataFrame(trained_models, columns=['pos_size', 'run', 'path']) \\\n",
    "    .astype({'run': 'int32' }) \\\n",
    "    .sort_values(by=['pos_size', 'run'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calc Results For All Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.5%': 30,\n",
       " '5%': 30,\n",
       " '2%': 30,\n",
       " '1%': 30,\n",
       " '40%': 30,\n",
       " '30%': 30,\n",
       " '20%': 30,\n",
       " '10%': 30,\n",
       " '90%': 30,\n",
       " '80%': 30,\n",
       " '70%': 30,\n",
       " '60%': 30,\n",
       " '50%': 30}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 0.5%\n",
      "Starting run 0\n"
     ]
    }
   ],
   "source": [
    "# make predictions with each trained model\n",
    "for (pos_size, run, weights_file) in trained_models.values:\n",
    "    if counts[pos_size] < 30:\n",
    "        continue\n",
    "    if run == 0:\n",
    "        print(f'Starting {pos_size}')\n",
    "    if run % 10 == 0:\n",
    "        print(f'Starting run {run}')\n",
    "   \n",
    "    model = load_model(model_json, weights_file)\n",
    "    \n",
    "    # optimal threshold is estimated using the training data\n",
    "    minority_size = float(pos_size.replace('%', ''))\n",
    "    delta = 0.005 if minority_size > 1 else 0.001\n",
    "    optimal_threshold = round(get_best_threshold(train_x, train_y, model, delta), 4)\n",
    "    \n",
    "    # theoretical threshold is the positive class prior\n",
    "    theoretical_threshold = minority_size / 100\n",
    "    \n",
    "    default_threshold = 0.5\n",
    "  \n",
    "    # record results\n",
    "    y_prob = model.predict(test_x)\n",
    "    write_performance_metrics(test_y, y_prob, minority_size, 'optimal', optimal_threshold, results_file)\n",
    "    write_performance_metrics(test_y, y_prob, minority_size, 'theoretical', theoretical_threshold, results_file)\n",
    "    write_performance_metrics(test_y, y_prob, minority_size, 'default', default_threshold, results_file)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
